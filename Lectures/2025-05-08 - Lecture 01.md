## Enhancing Touch Input through Machine Learning

Machine learning (ML) can be used to enrich the understanding of user input on capacitive touchscreens. Instead of only interpreting the 2D touch coordinate passed through the kernel of a smartphone, ML allows for the reconstruction of the **entire "blob"**—the real contact area detected by the display.

![[Pasted image 20250508112224.png]]

By analyzing this blob, ML models can extract features such as **pitch and yaw of the finger**—indicating the angle at which the finger touches the screen. This opens the door to more nuanced gestures and controls.

Moreover, ML can potentially identify **which finger is touching the screen**, enabling **multi-finger differentiation** for UI interactions. For instance:

> [!CITE] Finger-Specific Interaction Paradigm  
> "A finger could draw in blue in a drawing app, while another finger is mapped to draw in red."

This kind of interaction significantly enhances expressiveness in touch-based interfaces.

---

## Super-Resolution for Marker Detection

Capacitive displays have limited spatial resolution. However, with ML—particularly **Super Resolution techniques**—you can artificially enhance the resolution of capacitive input.

This becomes especially useful when trying to detect **fiducial markers**—physical tags placed on a screen that trigger specific UI responses.

![[Pasted image 20250508112430.png]]![[Pasted image 20250508112513.png]]

> [!TIP] Fiducial Marker  
> A visual pattern (e.g., a moon or sun) recognized by a system to trigger an event or identify a location.

ML is can be used to simulate and recognize these markers more efficiently.

---
## Adversarial Network (GAN) Simulations

> 🔍 **Generative Adversarial Networks (GANs):**  
> GANs consist of two neural networks—a **generator** and a **discriminator**—trained in opposition. The generator creates fake data, while the discriminator tries to distinguish real from fake. Over time, this adversarial training improves both.

In this context, GANs simulate how fiducial markers would look when pressed against a capacitive screen. Instead of capturing thousands of real examples for training, a **template + recording + simulation** pipeline is used:

|Step|Description|
|---|---|
|Template Design|A designer creates a visual marker (e.g., a sun shape).|
|Single Recording|One real capacitive input of the marker is recorded.|
|GAN Simulation|The generator creates many variations based on the initial recording.|

![[Pasted image 20250508112706.png]]


| Feature      | **Simulator Network**                         | **Recognizer Network**                          |
| ------------ | --------------------------------------------- | ----------------------------------------------- |
| **Input**    | Fiducial Marker image                         | Simulated or real capacitive image              |
| **Output**   | Simulated capacitive image                    | Predicted label or object ID                    |
| **Purpose**  | Forward model: _generate sensor response_     | Inverse model: _infer object from sensor data_  |
| **Type**     | Generative (e.g. GAN or CNN decoder)          | Discriminative (e.g. CNN classifier or encoder) |
| **Use Case** | Training data augmentation, sensor simulation | Object recognition, classification              |

These networks **decouple the design from the training**, meaning designers can create new markers without needing a new ML training cycle.

> [!TIP] One-Shot Learning  
> This is a technique where a model learns to recognize an object/class from a single example, ideal for scenarios with few available samples.

![[Pasted image 20250508123224.png]]

1. **Input - Fiducial Marker**
   A high-contrast, uniquely identifiable pattern (e.g., AprilTag or ArUco) is used to represent a known object or position. This visual marker serves as the input to the system.
2. **Step 2: Translation - Generator**
   A machine learning model (such as a neural network or GAN) takes the fiducial marker and generates a simulated representation of how it would affect a capacitive sensor.
3. **Step 3: Output - Simulated Capacitive Image**
   The output is a heatmap-like image that mimics the expected capacitive response. The colors indicate varying signal intensities, where brighter areas represent stronger capacitive effects.

> [!question] _How does the GAN-based simulation approach improve marker recognition?_  
> It removes the need for extensive real-world training data and allows for fast adaptation to new marker designs with minimal latency.

---
## Context / Gaze Aware Voice Assistants

Context-aware assistants combine multiple sensor inputs to interpret what a user might be focusing on. For example, a smartphone could:

- Use the **front-facing camera** to detect **head orientation** (gaze direction)
- Combine this with **rear-facing camera input** (scene view)
- Add **GPS data** and **environmental sensors**

This multimodal input helps the assistant infer the user’s intention. E.g., when someone looks at a café, the assistant might offer its opening hours.


![[Pasted image 20250508113700.png]]


> [!question] _How can gaze data enhance voice assistant interaction?_  
> It allows the assistant to proactively understand what the user is referencing, enabling more seamless and natural interactions in the real world.

---


## Dialog Systems and Human-Robot Collaboration

Dialog systems that operate in physical environments must **share control** with the user. This requires a negotiation process between human and machine.

![[Pasted image 20250508114047.png]]


> 🔍 **Negotiation Process:**  
> A method where the system and user align their goals through dialog. The system asks for clarifications, confirms understanding, and adapts its behavior accordingly.

![[Pasted image 20250508114025.png]]

These systems are often **Human-in-the-Loop**:

> 🔍 **Human-in-the-Loop Systems:**  
> Systems where humans provide feedback or make decisions during the process, improving the accuracy and safety of autonomous systems.

> [!question] _What is the role of negotiation in human-in-the-loop dialog systems?_  
> It helps align machine behavior with human intent, especially in dynamic, shared environments where assumptions must be confirmed.

---